# Bahdanau Attention

Our implementation of [*Bahdanau et al* traduction with an attention mechanism](https://arxiv.org/pdf/1409.0473.pdf), experimenting with translating english to Yoda.

We copy pasted a lot from:
* [Neural Machine Translation by Jointly Learning to Align and Translate](https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb) : Mainly for the models and the training
* [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) : for the data preparation
